---
title: "Homework-2"
header-includes:
  - \usepackage{lscape}
  - \usepackage[english]{babel}
  - \usepackage{multicol}
  - \usepackage{here}
  - \usepackage[version=3]{mhchem}
  - \setlength{\columnsep}{1cm}
  - \setlength{\parindent}{1cm} # paragraph indentation
  - \setlength{\parskip}{8pt} # paragraph spacing
  - \usepackage{setspace}
  - \doublespacing
output:
  html_document:
    df_print: paged
  pdf_document: default
  word_document: default
classoption: fleqn
---
#Smeerah Talafha
```{r}
#setwd("~/Desktop/r-583")
```

## Question-8.18
# a)


```{r}
data=read.table("T1-91.txt",header=T)
head(data)
```




```{r}
#install.packages("robustHD")
library(robustHD)
#x<-standardize(data[,2:8],centerFun = mean, scaleFun = sd)
#data[,2:8]<-(-1)*data[,2:8]#resign lower scores correspond better berformane
x<-data[,2:8] 
cor(x)
```



```{r}

#x <- cbind(X1_100m,X2_200m, X3_400m,X4_800m,X5_1500m,X6_3000m, Marathan ) 
A<-cor(x)
Eigenvalues <- eigen(A)$values
cat("Eigenvalues are: \n")
Eigenvalues
#A
```


```{r}
Eigenvectors <- eigen(A)$vectors
cat("Eigenvectors are: \n")
Eigenvectors
```




```{r}
p1 <- princomp(x, cor = TRUE)  ## using correlation matrix
summary(p1)
plot(p1)
screeplot(p1, npcs=7, type="lines")
```


#b)
According to the outputs above in the part(a), the first two principal combonents for the standardized variables is given as below. 

\[y_{1}^{^}= e^{^}_{1}X=.378X1+.383X2+.368X3+.395X4+.389X5+.376X6+.355X7 \]
\[y_{2}^{^}= e^{^}_{2}X=-.407X1-.414X2-.459X3+.161X4+.309X5+.423X6+.389X7 \]
According to the above results,the total sample variance explained by the first two sample variances is  approximately 91.96% of the total sample variance.



```{r}
# ++++++++++++++++++++++++++++
# flattenCorrMatrix
# ++++++++++++++++++++++++++++
# cormat : matrix of the correlation coefficients
# pmat : matrix of the correlation p-values
flattenCorrMatrix <- function(cormat, pmat) {
  ut <- upper.tri(cormat)
  data.frame(
    row = rownames(cormat)[row(cormat)[ut]],
    column = rownames(cormat)[col(cormat)[ut]],
    cor  =(cormat)[ut],
    p = pmat[ut]
    )
}
```




```{r}
#install.packages("Hmisc")
library("Hmisc")
S<-rcorr(as.matrix(x), type = c("pearson"))
flattenCorrMatrix(S$r, S$P)
```

according the results above , it can be seen that the all event contrbuite equally of the first component which might be called as the way index.The second components contrasts the times for the shorter distance and the longer distance so it can be called as the distance component.


# d)

```{r}
data$scores <- p1$scores[,1]
df_sorted_names_asc <- data[with(data, order(scores)), ] 
y<-df_sorted_names_asc[c(1,9)]
z<-na.omit(y)
z
```

according to above ranking it can be seen that the rankings appears consistent with intuitive notions of the athletic excellence.



****************************************************************************************************************************************************************************************************************************************************************
  
## Question-8-19




```{r}
data=read.table("T1-91.txt",header=T)
data
```

```{r}
#convert speed in meters per second
data$X1_100m=100/data$X1_100m;
data$X2_200m=200/data$X2_200m;
data$X3_400m=400/data$X3_400m;
data$X4_800m=800/(data$X4_800m*60);
data$X5_1500m=1500/(data$X5_1500m*60);
data$X6_3000m=3000/(data$X6_3000m*60);
data$Marathan=42195/(data$Marathan*60);
head(data)
```






```{r}
#install.packages("robustHD")
library(robustHD)
#x<-standardize(data[,2:8],centerFun = mean, scaleFun = sd)
x<-data[,2:8]
cov(x)
```



```{r}

#x <- cbind(X1_100m,X2_200m, X3_400m,X4_800m,X5_1500m,X6_3000m, Marathan ) 
A<-cov(x)
Eigenvalues <- eigen(A)$values
cat("Eigenvalues are: \n")
Eigenvalues
#A
```


```{r}
Eigenvectors <- eigen(A)$vectors
cat("Eigenvectors are: \n")
Eigenvectors
```


```{r}
p1 <- princomp(x)  ## using covariance matrix
summary(p1)
plot(p1)
screeplot(p1, npcs=7, type="lines")
```

According to the outputs above in the part(a), the first two principal combonents for the standardized variables is given as below. 

\[y_{1}^{^}= e^{^}_{1}X=.310X1+.357X2+.378X3+.299X4+.391X5+.459X6+.422X7 \]
\[y_{2}^{^}= e^{^}_{2}X=.375X1+.433X2+.518X3-.053X4+.210X5-.395X6-.444X7 \]
According to the above results,the total sample variance explained by the first two sample variances is  approximately 92.59% of the total sample variance.it can be seen that the all event conrbute equally of the first component which might be called as the way index.the second components contrasts the times for the shorter distance and the longer distance so it can be called as the distance component.Thus, the above interpretation of the covariance matrix is smilar as the results obtained in Exercise 8.18 of the text book.





```{r}
# ++++++++++++++++++++++++++++
# flattenCorrMatrix
# ++++++++++++++++++++++++++++
# cormat : matrix of the correlation coefficients
# pmat : matrix of the correlation p-values
flattenCorrMatrix <- function(cormat, pmat) {
  ut <- upper.tri(cormat)
  data.frame(
    row = rownames(cormat)[row(cormat)[ut]],
    column = rownames(cormat)[col(cormat)[ut]],
    cor  =(cormat)[ut],
    p = pmat[ut]
    )
}
```




```{r}
#install.packages("Hmisc")
library("Hmisc")
S<-rcorr(as.matrix(x), type = c("pearson","spearman"))
flattenCorrMatrix(S$r, S$P)
```



```{r}
data$scores <- p1$scores[,1]
df_sorted_names_asc <- data[with(data, order(scores)), ] 
y<-df_sorted_names_asc[c(1,9)]
z<-na.omit(y)
z
```
According to the above ranking it can be seen that rankings appears  consistent with intuitive notions of the athletic excellence. Thus, the above obtained rankings of the countries basis of the first principal components score are differ from the rankings obtained in the Exercise 8.18 of the text book. 


The total variance explained by the first two principal components for the covariance matrix is approximately 92.59% which indicates the data summarized in the two dimensions. The total variance explained by the first two principle components in the Exercise 8.18 of the textbook is approximately 91.9% which also indicates the data is summarized in the two dimensions. so the total variance explained by the first PCs in the covariance matrix is greater than the total variance explained by two PCs in the Exercise 8.18 which indicates that the covariance matrix can be used for analysis procedure.

****************************************************************************************************************************************************************************************************************************************************************
## Question-8-20

```{r}
X100m=matrix(c(10.23,9.93,10.15,10.14,10.27,10.00,9.84	,10.10,10.17,10.29,10.97,10.32,10.24,10.29,10.16,10.21,10.02,10.06,9.87,10.11,10.32,10.08,10.33,10.20,10.35,10.20,10.01,10.00,10.28,10.34,10.60,10.41,10.30,10.13,10.21,10.64,10.19,10.11,10.08,10.40,10.57,10.00,9.86,10.21,10.11,10.78,10.37,10.17,10.18,10.16,10.36,10.23,10.38,9.78),ncol=1,byrow=TRUE)

X200m=matrix(c(20.4, 20.1 ,20.4,20.2 ,20.3 ,19.9, 20.2 ,20.1, 20.4 ,20.9, 22.5 ,21.0 ,20.6 ,20.5 ,20.6 ,20.5, 20.2 ,20.2 ,19.9 ,19.9, 21.1 ,20.1, 20.7 ,20.9, 20.5, 20.9, 19.7, 20.0, 20.4, 20.4 ,21.2 ,20.8, 20.9 ,20.1, 20.4 ,21.5, 20.2 ,20.4 ,20.2, 21.2, 21.4 ,20.0 ,20.1, 20.8 ,20.2 ,21.9, 21.1, 20.6 ,20.4 ,20.4, 20.8, 20.7, 21.0 ,19.3),ncol=1,byrow=TRUE)
X400m=matrix(c( 46.2 ,44.4 ,45.8 ,45.0 ,45.3, 44.3, 44.7, 45.9, 45.2, 45.8, 51.4, 46.4, 45.8, 45.9 ,44.9, 45.5, 44.6 ,44.3, 44.4, 45.6, 48.4, 45.4 ,45.5 ,46.4 ,45.6, 46.6 ,45.3, 44.8, 44.2 ,45.4 ,47.0 ,47.9, 46.4 ,44.7, 44.3, 48.6, 45.7, 46.1 ,46.1 ,46.8, 45.6 ,44.6, 46.1 ,45.8 ,44.6 ,50.0 ,47.6, 45.0 ,45.5 ,45.0 ,46.7, 46.0 ,46.6 ,43.2),ncol=1,byrow=TRUE)

X800m=matrix(c( 1.77, 1.74 ,1.77 ,1.73, 1.79 ,1.70, 1.75 ,1.76 ,1.77, 1.80 ,1.94 ,1.87 ,1.75, 1.69 ,1.81 ,1.74 ,1.72 ,1.73 ,1.70 ,1.75, 1.82 ,1.76 ,1.76 ,1.83, 1.75, 1.80, 1.73 ,1.77, 1.70, 1.74, 1.82 ,1.76 ,1.79, 1.80 ,1.78 ,1.80 ,1.73 ,1.74, 1.71, 1.80, 1.80 ,1.72, 1.75 ,1.76 ,1.71 ,1.94 ,1.84 ,1.73, 1.76, 1.71, 1.79, 1.81 ,1.78 ,1.71),ncol=1,byrow=TRUE)

X1500m=matrix(c( 3.68 ,3.53 ,3.58 ,3.57 ,3.70 ,3.57 ,3.53, 3.65, 3.61, 3.72, 4.24, 3.84 ,3.58, 3.52, 3.73, 3.61, 3.48, 3.53, 3.49 ,3.61, 3.74 ,3.59 ,3.63 ,3.77 ,3.56 ,3.70, 3.35, 3.62, 3.44 ,3.64, 3.77, 3.67, 3.76, 3.83, 3.63, 3.80 ,3.55, 3.54 ,3.62 ,4.00, 3.82 ,3.59 ,3.50 ,3.57, 3.54 ,4.01 ,3.86 ,3.48 ,3.61 ,3.53, 3.77, 3.77, 3.59, 3.46),ncol=1,byrow=TRUE)

X5000m=matrix(c(13.33,  12.93 , 13.26  ,12.83  ,14.64 , 13.48  ,13.23,  13.39 , 13.42 , 13.49  ,16.7,   13.75  ,13.42 , 13.42 , 14.31 , 13.27 , 12.98 , 12.91 , 13.01  ,13.48 , 13.98 , 13.45 , 13.5 ,  14.21 , 13.07 , 13.66,  13.09,  13.22,  12.66 , 13.84,  13.9 ,  13.64,  14.11 , 14.15 , 13.13 , 14.19 , 13.22 , 13.21,  13.11  ,14.72 , 13.97,  13.29 , 13.05  ,13.25, 13.2,   16.28,  14.96 , 13.04 , 13.29,  13.13,  13.91,  14.25 , 13.45,  12.97 ),ncol=1,byrow=TRUE)


X10000m=matrix(c(27.6 ,27.5, 27.7 ,26.9 ,30.5 ,28.1 ,27.6 ,28.1, 28.2, 27.9, 35.4 ,28.8 ,27.8 ,27.9, 30.4 ,27.5, 27.4 ,27.4, 27.3 ,28.1 ,29.3 ,28.0 ,28.8, 29.6 ,27.8 ,28.7 ,27.3 ,27.6 ,26.5, 28.5 ,28.4, 28.8 ,29.5, 29.8 ,27.1 ,29.6, 27.4, 27.7 ,27.5 ,31.4, 29.0, 27.9 ,27.2 ,27.7, 27.9 ,34.7 ,31.3 ,27.2 ,27.9 ,27.9, 29.2 ,29.7, 28.3 ,27.2),ncol=1,byrow=TRUE)

Marathon=matrix(c(130 ,128, 132, 127, 146 ,126, 130 ,132 ,129 ,131 ,171 ,133 ,132, 129 ,146 ,131 ,126, 128, 127 ,132, 133, 132, 132 ,139 ,129 ,134 ,127 ,126 ,125 ,127, 129 ,134 ,149 ,143, 127 ,140 ,128 ,129 ,130, 148 ,138 ,129, 126, 132, 129, 162 ,144 ,127, 130 ,130 ,134, 139, 130, 125),ncol=1,byrow=TRUE)
Country=matrix(c("Argentina","Australia","Austria","Belgium","Bermuda","Brazil","Canada","Chile","China","Columbia","CookIslands","CostaRica","CzechRepublic", "Denmark","DominicanRepub", "Finland "," France"  , "Germany"  ,"GreatBritain" ,  "Greece",        " Guatemala",'Hungary',"India","Indonesia","Ireland","Israel" ,"Italy","Japan",  "Kenya","KoreaSouth", "KoreaNorth",   "Luxembourg"  ,    "Malaysia"  ,      "Mauritius"    ,  "Mexico" ,         "Myanmar(Burma)",  "Netherlands"  ,  "NewZealand",      "Norway","PapuaNewGuinea","Philippines","Poland","Portugal"     , "Romania"  ,      "Russia",  "Samoa"         , "Singapore",      "Spain",          " Sweden"        ,  "Switzerland"   , "Taiwan"      , "Thailand"  ,   "TUrkey",          "U.S.A." ),ncol=1,byrow=TRUE)

data<- data.frame(Country=Country,X100m=X100m,X200m=X200m,X400m=X400m,X800m=X800m,X1500m=X1500m,X5000m=X5000m,X10000m=X10000m,Marathon=Marathon)

head(data)
     

```








```{r}
#install.packages("robustHD")
library(robustHD)
#x<-standardize(data[,2:8],centerFun = mean, scaleFun = sd)
x<-data[,2:9]
cor(x)
```



```{r}

#x <- cbind(X1_100m,X2_200m, X3_400m,X4_800m,X5_1500m,X6_3000m, Marathan ) 
A<-cor(x)
Eigenvalues <- eigen(A)$values
cat("Eigenvalues are: \n")
Eigenvalues
#A
```


```{r}
Eigenvectors <- eigen(A)$vectors
cat("Eigenvectors are: \n")
Eigenvectors
```


```{r}
p1 <- princomp(x ,cor = TRUE)  ## using covariance matrix
summary(p1)
plot(p1)
screeplot(p1, npcs=8, type="lines")
```


According to the outputs above in the part(a), the first two principal combonents for the standardized variables is given as below. 

\[y_{1}^{^}= e^{^}_{1}X=.332X1+.346X2+.339X3+.353X4+.366X5+.370X6+.366X7.354X8 \]
\[y_{2}^{^}= e^{^}_{2}X=.52X1+.47X2+.345X3-.089X4-.154X5-.295X6-.334X7-.387X8 \]
According to the above results,the total sample variance explained by the first two sample variances is  approximately 91.61% of the total sample variance. approximately  92.34 %  the total sample variance . 



```{r}
# ++++++++++++++++++++++++++++
# flattenCorrMatrix
# ++++++++++++++++++++++++++++
# cormat : matrix of the correlation coefficients
# pmat : matrix of the correlation p-values
flattenCorrMatrix <- function(cormat, pmat) {
  ut <- upper.tri(cormat)
  data.frame(
    row = rownames(cormat)[row(cormat)[ut]],
    column = rownames(cormat)[col(cormat)[ut]],
    cor  =(cormat)[ut],
    p = pmat[ut]
    )
}
```




```{r}
#install.packages("Hmisc")
library("Hmisc")
S<-rcorr(as.matrix(x), type = c("pearson"))
flattenCorrMatrix(S$r, S$P)
```






```{r}
data$scores <- p1$scores[,1]
df_sorted_names_asc <- data[with(data, order(scores)), ] 
y<-df_sorted_names_asc[c(1,10)]
z<-na.omit(y)
z
```


************************************************************************************************************************************************************************
## Question-8-21

```{r}
X100m=matrix(c(10.23,9.93,10.15,10.14,10.27,10.00,9.84	,10.10,10.17,10.29,10.97,10.32,10.24,10.29,10.16,10.21,10.02,10.06,9.87,10.11,10.32,10.08,10.33,10.20,10.35,10.20,10.01,10.00,10.28,10.34,10.60,10.41,10.30,10.13,10.21,10.64,10.19,10.11,10.08,10.40,10.57,10.00,9.86,10.21,10.11,10.78,10.37,10.17,10.18,10.16,10.36,10.23,10.38,9.78),ncol=1,byrow=TRUE)

X200m=matrix(c(20.4, 20.1 ,20.4,20.2 ,20.3 ,19.9, 20.2 ,20.1, 20.4 ,20.9, 22.5 ,21.0 ,20.6 ,20.5 ,20.6 ,20.5, 20.2 ,20.2 ,19.9 ,19.9, 21.1 ,20.1, 20.7 ,20.9, 20.5, 20.9, 19.7, 20.0, 20.4, 20.4 ,21.2 ,20.8, 20.9 ,20.1, 20.4 ,21.5, 20.2 ,20.4 ,20.2, 21.2, 21.4 ,20.0 ,20.1, 20.8 ,20.2 ,21.9, 21.1, 20.6 ,20.4 ,20.4, 20.8, 20.7, 21.0 ,19.3),ncol=1,byrow=TRUE)
X400m=matrix(c( 46.2 ,44.4 ,45.8 ,45.0 ,45.3, 44.3, 44.7, 45.9, 45.2, 45.8, 51.4, 46.4, 45.8, 45.9 ,44.9, 45.5, 44.6 ,44.3, 44.4, 45.6, 48.4, 45.4 ,45.5 ,46.4 ,45.6, 46.6 ,45.3, 44.8, 44.2 ,45.4 ,47.0 ,47.9, 46.4 ,44.7, 44.3, 48.6, 45.7, 46.1 ,46.1 ,46.8, 45.6 ,44.6, 46.1 ,45.8 ,44.6 ,50.0 ,47.6, 45.0 ,45.5 ,45.0 ,46.7, 46.0 ,46.6 ,43.2),ncol=1,byrow=TRUE)

X800m=matrix(c( 1.77, 1.74 ,1.77 ,1.73, 1.79 ,1.70, 1.75 ,1.76 ,1.77, 1.80 ,1.94 ,1.87 ,1.75, 1.69 ,1.81 ,1.74 ,1.72 ,1.73 ,1.70 ,1.75, 1.82 ,1.76 ,1.76 ,1.83, 1.75, 1.80, 1.73 ,1.77, 1.70, 1.74, 1.82 ,1.76 ,1.79, 1.80 ,1.78 ,1.80 ,1.73 ,1.74, 1.71, 1.80, 1.80 ,1.72, 1.75 ,1.76 ,1.71 ,1.94 ,1.84 ,1.73, 1.76, 1.71, 1.79, 1.81 ,1.78 ,1.71),ncol=1,byrow=TRUE)

X1500m=matrix(c( 3.68 ,3.53 ,3.58 ,3.57 ,3.70 ,3.57 ,3.53, 3.65, 3.61, 3.72, 4.24, 3.84 ,3.58, 3.52, 3.73, 3.61, 3.48, 3.53, 3.49 ,3.61, 3.74 ,3.59 ,3.63 ,3.77 ,3.56 ,3.70, 3.35, 3.62, 3.44 ,3.64, 3.77, 3.67, 3.76, 3.83, 3.63, 3.80 ,3.55, 3.54 ,3.62 ,4.00, 3.82 ,3.59 ,3.50 ,3.57, 3.54 ,4.01 ,3.86 ,3.48 ,3.61 ,3.53, 3.77, 3.77, 3.59, 3.46),ncol=1,byrow=TRUE)

X5000m=matrix(c(13.33,  12.93 , 13.26  ,12.83  ,14.64 , 13.48  ,13.23,  13.39 , 13.42 , 13.49  ,16.7,   13.75  ,13.42 , 13.42 , 14.31 , 13.27 , 12.98 , 12.91 , 13.01  ,13.48 , 13.98 , 13.45 , 13.5 ,  14.21 , 13.07 , 13.66,  13.09,  13.22,  12.66 , 13.84,  13.9 ,  13.64,  14.11 , 14.15 , 13.13 , 14.19 , 13.22 , 13.21,  13.11  ,14.72 , 13.97,  13.29 , 13.05  ,13.25, 13.2,   16.28,  14.96 , 13.04 , 13.29,  13.13,  13.91,  14.25 , 13.45,  12.97 ),ncol=1,byrow=TRUE)


X10000m=matrix(c(27.6 ,27.5, 27.7 ,26.9 ,30.5 ,28.1 ,27.6 ,28.1, 28.2, 27.9, 35.4 ,28.8 ,27.8 ,27.9, 30.4 ,27.5, 27.4 ,27.4, 27.3 ,28.1 ,29.3 ,28.0 ,28.8, 29.6 ,27.8 ,28.7 ,27.3 ,27.6 ,26.5, 28.5 ,28.4, 28.8 ,29.5, 29.8 ,27.1 ,29.6, 27.4, 27.7 ,27.5 ,31.4, 29.0, 27.9 ,27.2 ,27.7, 27.9 ,34.7 ,31.3 ,27.2 ,27.9 ,27.9, 29.2 ,29.7, 28.3 ,27.2),ncol=1,byrow=TRUE)

Marathon=matrix(c(130 ,128, 132, 127, 146 ,126, 130 ,132 ,129 ,131 ,171 ,133 ,132, 129 ,146 ,131 ,126, 128, 127 ,132, 133, 132, 132 ,139 ,129 ,134 ,127 ,126 ,125 ,127, 129 ,134 ,149 ,143, 127 ,140 ,128 ,129 ,130, 148 ,138 ,129, 126, 132, 129, 162 ,144 ,127, 130 ,130 ,134, 139, 130, 125),ncol=1,byrow=TRUE)
Country=matrix(c("Argentina","Australia","Austria","Belgium","Bermuda","Brazil","Canada","Chile","China","Columbia","CookIslands","CostaRica","CzechRepublic", "Denmark","DominicanRepub", "Finland "," France"  , "Germany"  ,"GreatBritain" ,  "Greece",        "Guatemala",'Hungary',"India","Indonesia","Ireland","Israel" ,"Italy","Japan",  "Kenya","KoreaSouth", "KoreaNorth",   "Luxembourg"  ,    "Malaysia"  ,      "Mauritius"    ,  "Mexico" ,         "Myanmar(Burma)",  "Netherlands"  ,  "NewZealand",      "Norway","PapuaNewGuinea","Philippines","Poland","Portugal"     , "Romania"  ,      "Russia",  "Samoa"         , "Singapore",      "Spain",          " Sweden"        ,  "Switzerland"   , "Taiwan"      , "Thailand"  ,   "TUrkey",          "U.S.A." ),ncol=1,byrow=TRUE)

data<- data.frame(Country=Country,X100m=X100m,X200m=X200m,X400m=X400m,X800m=X800m,X1500m=X1500m,X5000m=X5000m,X10000m=X10000m,Marathon=Marathon)

data

```

```{r}
#convert speed in meters per second
data$X100m=100/data$X100m;
data$X200m=200/data$X200m;
data$X400m=400/data$X400m;
data$X800m=800/(data$X800m*60);
data$X1500m=1500/(data$X1500m*60);
data$X5000m=5000/(data$X5000m*60);
data$X10000m=10000/(data$X10000m*60);
data$Marathon=42195/(data$Marathon*60);
head(data)
```






```{r}
#install.packages("robustHD")
library(robustHD)
#x<-standardize(data[,2:8],centerFun = mean, scaleFun = sd)
x<-data[,2:9]
cov(x)
```



```{r}

#x <- cbind(X1_100m,X2_200m, X3_400m,X4_800m,X5_1500m,X6_3000m, Marathan ) 
A<-cov(x)
Eigenvalues <- eigen(A)$values
cat("Eigenvalues are: \n")
Eigenvalues
#A
```


```{r}
Eigenvectors <- eigen(A)$vectors
cat("Eigenvectors are: \n")
Eigenvectors
```


```{r}
p1 <- princomp(x)  ## using covariance matrix
summary(p1)
plot(p1)
screeplot(p1, npcs=8, type="lines")
```


According to the outputs above in the part(a), the first two principal combonents for the standardized variables is given as below. 

\[y_{1}^{^}= e^{^}_{1}X=.224X1+.311X2+.317X3+.278X4+.364X5+.428X6+.421X7+.416X8 \]
\[y_{2}^{^}= e^{^}_{2}X=.432X1+.523X2+.469X3+.033X4-.062X5-.261X6-.309X7-.386X8 \]


According to the above results,the total sample variance explained by the first two sample variances is  approximately 92.06% of the total sample variance. approximately 92.06 % the total sample variance . it can be seen that the all event contributed equally of the first component which might be called as the way index. the second components  contrasts the times for the shorter distance and the longer distance so it can be called as the distance component . this above interpretation of the covariance matrix is similar as the results obtained in Exercise 8.20



```{r}
# ++++++++++++++++++++++++++++
# flattenCorrMatrix
# ++++++++++++++++++++++++++++
# cormat : matrix of the correlation coefficients
# pmat : matrix of the correlation p-values
flattenCorrMatrix <- function(cormat, pmat) {
  ut <- upper.tri(cormat)
  data.frame(
    row = rownames(cormat)[row(cormat)[ut]],
    column = rownames(cormat)[col(cormat)[ut]],
    cor  =(cormat)[ut],
    p = pmat[ut]
    )
}
```




```{r}
#install.packages("Hmisc")
library("Hmisc")
S<-rcorr(as.matrix(x), type = c("pearson"))
flattenCorrMatrix(S$r, S$P)
```






```{r}
data$scores <- p1$scores[,1]
df_sorted_names_asc <- data[with(data, order(scores)), ] 
y<-df_sorted_names_asc[c(1,10)]
z<-na.omit(y)
z
```

According to the above ranking it can be seen that the rankings appears consistent with intuitive  notions of the athletic excellence . Thus the above obtain rankings of the countries basis of the first principal components score are differ from the rankings obtained in the Exercise 8.20 of the text book.


According to the above results, the total variance explained by the first two principal components for the covariance matrix approximately 92.06 % which indicates the data is summarized in the two dimensions. the total variance explained the first two PCs in Exercise 8.20 is approximately 91.61% which also indicates the data is summarized in the two dimensions. so the total variance explained by the first two PCs in the covariance matrix is greater than the total variance explained by the first two PCS in the Exercise 8.20 which indicates that the covariance matrix can be used for analysis procedure.
****************************************************************************************************************************************************************************************************************************************************************
## Question-8-22


# a)  first using correlation matrix 

```{r}
data1=read.table("T1-10.txt",header=T)
data1
```




```{r}
#install.packages("robustHD")
library(robustHD)
#x<-standardize(data[,2:8],centerFun = mean, scaleFun = sd)
x<-data1[,3:9]
cor(x)
```



```{r}

#x <- cbind(X1_100m,X2_200m, X3_400m,X4_800m,X5_1500m,X6_3000m, Marathan ) 
A<-cor(x)
Eigenvalues <- eigen(A)$values
cat("Eigenvalues are: \n")
Eigenvalues
#A
```


```{r}
Eigenvectors <- eigen(A)$vectors
cat("Eigenvectors are: \n")
Eigenvectors
```


```{r}
p1 <- princomp(x, cor = TRUE)  ## using correlation matrix
summary(p1)
plot(p1)
screeplot(p1, npcs=7, type="lines")
```


```{r}
z<-x$scores <- p1$scores

z[,1:2]
```





```{r}
#install.packages("ggfortify")
library(ggfortify)
autoplot(prcomp(data1[,3:9],scale = TRUE), data = data1,colour = 'Breed', shape = TRUE, label.size = 3,  loadings.colour = 'blue',
         loadings.label = TRUE, loadings.label.size = 3, outlier.color = "red")
```


```{r}
library("car")
pca<-prcomp(data1[,3:9],scale = TRUE)
qqPlot(pca$x[,1],pch = 20, col = c(rep("red", 33), rep("blue", 99)))
```



b)  using covariance matrix 

```{r}
data1=read.table("T1-10.txt",header=T)
data1
```




```{r}
#install.packages("robustHD")
library(robustHD)
#x<-standardize(data[,2:8],centerFun = mean, scaleFun = sd)
x<-data1[,3:9]
cov(x)
```



```{r}

#x <- cbind(X1_100m,X2_200m, X3_400m,X4_800m,X5_1500m,X6_3000m, Marathan ) 
A<-cov(x)
Eigenvalues <- eigen(A)$values
cat("Eigenvalues are: \n")
Eigenvalues
#A
```


```{r}
Eigenvectors <- eigen(A)$vectors
cat("Eigenvectors are: \n")
Eigenvectors
```


```{r}
p1 <- princomp(x)  ## using correlation matrix
summary(p1)
plot(p1)
screeplot(p1, npcs=7, type="lines")
```









```{r}
z<-x$scores <- p1$scores
z[,1:2]
```

```{r}
#install.packages("ggfortify")
library(ggfortify)
autoplot(prcomp(data1[,3:9]), data = data1,colour = 'Breed', shape = TRUE, label.size = 3,  loadings.colour = 'blue',
         loadings.label = TRUE, loadings.label.size = 3 ,outlier.color = "red")
```




```{r}
library("car")
pca<-prcomp(data1[,3:9])
qqPlot(pca$x[,1],pch = 20, col = c(rep("red", 33), rep("blue", 99)))
```


there is one point outlier


****************************************************************************************************************************************************************************************************************************************************************

## Question 9-33





```{r}
data=read.table("T4-6.txt",header=T)
data
data$lndep  <- as.numeric(data$lndep )
data$Supp<- as.numeric(data$Supp)
data$Benev  <- as.numeric(data$Benev )
data$Conform <- as.numeric(data$Conform)
data$Leader <- as.numeric(data$Leader)

```


**factor Analysis using PCA 
*number of factor is two rotate and non rotate



```{r}
#install.packages("psych")
#install.packages("GPArotation") 
library(psych)
library(GPArotation)
```

```{r}
fit <- fa((data[,1:5]), nfactors=2,rotate = "none", fm='pa',scores="tenBerge",cor="cor")
print(fit$loading) # print results
```



```{r}
factor <- fa(data[,1:5],nfactors = 2,rotate = "varimax", fm='pa',scores="tenBerge",cor="cor")
factor$loading
```

```{r}
##install.packages("psych")
##install.packages("GPArotation")
library(psych)
library(GPArotation)
fit <- fa.parallel(data[,1:5], nfactors=2, fm='pa')
print(fit) # print results
```









```{r}
fa.diagram(factor)
```

```{r}
##install.packages("psych")
##install.packages("GPArotation")
library(psych)
library(GPArotation)
fit <- fa(data[,1:5], nfactors=3,rotate = "none", fm='pa',scores="tenBerge",cor="cor")
print(fit$loading) # print results
```
```{r}
threefactor <- fa(data[,1:5],nfactors =3,rotate = "varimax", fm='pa',scores="tenBerge",cor="cor")
threefactor$loading
```



```{r}
##install.packages("psych")
##install.packages("GPArotation")
library(psych)
library(GPArotation)
fit <- fa.parallel(data[,1:5], nfactors=3, fm='pa', fa='fa')
print(fit) # print results
```


```{r}
fa.diagram(threefactor)
```






```{r}
#install.packages("nFactors")
library(nFactors)
ev <- eigen(cor(data[,1:5])) 
ap <- parallel(subject=nrow(data[,1:5]),var=ncol(data[,1:5]), rep=100, cent=.05)
nS <- nScree(x=ev$values, aparallel=ap$eigen$qevpea)

plotnScree(nS)
```

**factor Analysis using maximum liklihood
*number of factor is two rotate and non rotate




```{r}
fit.2 <- factanal(data[,1:5],factors=2,rotation="varimax" ,scores = "regression")
print(fit.2, digits = 2, cutoff = .2, sort = TRUE)
```
```{r}
fit <- factanal(cor(data[,1:5]), factors=3,rotation='none')
print(fit, digits=2, cutoff=.3, sort=TRUE)
```

```{r}
fit<- factanal(data[,1:5],factors=3,rotation="varimax")
print(fit, digits=2, cutoff=.3, sort=TRUE)
```





the result by the two methods, that is PCA and ML shows that the latter is diffecult to interpret . the factor scores for the first two factors are the smilar for the two solution. 


****************************************************************************************************************************************************************************************************************************************************************

##Question 10.9
```{r}
R11=matrix(c(1,0.6328,0.6328,1),ncol=2,byrow=TRUE)
R12=matrix(c(0.2412,0.0586,-0.0553,0.0655),ncol=2,byrow=TRUE)
R21=matrix(c(0.2412,-.0553,0.0586,0.0655),ncol=2,byrow=TRUE)
R22=matrix(c(1,0.4248,.4248,1),ncol=2,byrow=TRUE)
```









## a)

```{r}
# Finding the E1 and E2 matrices:
E1 <- solve(R11) %*% R12 %*% solve(R22) %*% R21
E2 <- solve(R22) %*% R21 %*% solve(R11) %*% R12
```


```{r}
# The canonical correlations are:
Xeig <- sqrt(eigen(E1)$values)
Yeig <- sqrt(eigen(E2)$values)
cat(" ### Canonical Correlation \n")
Xeig
```
```{r}
cat("# The canonical variates are based on the eigenvectors of E1 :\n\n")
eigen(E1)$vectors 
cat("\n# a1 = (0.7748662 ,-0.6321252)","\n") 
cat("# b1 = (0.3537872, 0.9353259) \n \n")

cat("# The canonical variates are based on the eigenvectors of E2 :\n\n")
eigen(E2)$vectors 
cat("\n# a2 = (0.9252848 ,-0.3792729 )","\n") 
cat("# b2 = ( -0.01804025, 0.99983726)\n")

```

```{r}
cat("# u1 = 0.7748662*reading.power - 0.6321252*reading.space \n")
cat("# u2 = 0.3537872*reading.power + 0.9353259*reading.space \n")
cat("# v1 = 0.9252848 *arethmetic.power -0.3792729*arethmetic.space\n")
cat("# v2 = -0.01804025*arethmetic.power + 0.99983726*arethmetic.space")

```








## b)
```{r}
## Bartlett's test for the significance of the first canonical correlation:
## The null hypothesis is that the first (and smaller) canonical correlations are zero.

my.n <- 140; my.q1 <- 2; my.q2 <- 2
test.stat <- -( (my.n-1) - 0.5*(my.q1+my.q2+1) ) * sum(log(1-eigen(E1)$values))
test.stat
P.value <- pchisq(test.stat, df = my.q1*my.q2, lower.tail=F)
P.value
```

** Since the P-value is tiny, we conclude that there is at least one 
** nonzero canonical correlation.

```{r}
## Bartlett's test for the significance of the second canonical correlation:
## The null hypothesis is that the second (and smaller) canonical correlations are zero in general, 
## but there's only two here.

my.n <- 140; my.q1 <- 2; my.q2 <- 2
test.stat <- -( (my.n-1) - 0.5*(my.q1+my.q2+1) ) * sum(log(1-eigen(E1)$values[-1]))
test.stat
P.value <- pchisq(test.stat, df = (my.q1-1)*(my.q2-1), lower.tail=F)
P.value

```

** The P-value is again small, so we conclude there are at least two 
** nonzero canonical correlations.  In this case, that means exactly two




## c)

```{r}
u1 <- t(solve(sqrt(R11))) %*% (eigen(E1)$vectors[,1])
v1 <- solve(sqrt(R22)) %*% (eigen(E2)$vectors[,1])
plot(u1,v1)
cor(u1,v1)
```


```{r}
u2<- solve(sqrt(R11)) %*% (eigen(E1)$vectors[,2])
v2 <- solve(sqrt(R22)) %*% (eigen(E2)$vectors[,2])
plot(u2,v2)
cor(u2,v2)
```

## Another method


```{r}


R1=matrix(c(1 ,.615,.615, 1),2,2)

R12=matrix(c(-.111,-.266,-.195,-.085),2,2,byrow=T)

R2=matrix(c(1,-.269,-.269,1),2,2)

R1eig=eigen(R1,symmetric = T)

R1sqrt=R1eig$vectors%*%diag(1/sqrt(R1eig$values))%*%t(R1eig$vectors)

R2eig=eigen(R2,symmetric = T)

R2sqrt=R2eig$vectors%*%diag(1/sqrt(R2eig$values))%*%t(R2eig$vectors)

R12eig=eigen(R12,symmetric = T)

#R12sqrt=R12eig$vectors%*%diag(1/sqrt(R12eig$values))%*%t(R12eig$vectors)

Umat=R1sqrt%*%R12%*%solve(R2)%*%t(R12)%*%R1sqrt

Vmat=R2sqrt%*%t(R12)%*%solve(R1)%*%(R12)%*%R2sqrt

Ueig=eigen(Umat,symmetric = T)

Veig=eigen(Vmat,symmetric = T)

rho=sqrt(Ueig$values)



rho

```
## [1] 0.3266219 0.1710696

H0 : _ = 0 vs H1 : not H0

```{r}
statistic=-(140-1-(0.5)*(2+2+1))*(log(1-rho[1]^2)+log(1-rho[2]^2))

1-pchisq(statistic,4)
```



## [1] 0.000640116

Null hypothesis rejected at 5% significant level.



*********************************************************************************************************************************************
























